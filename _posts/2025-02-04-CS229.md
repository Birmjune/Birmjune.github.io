---
title: CS229
date: 2025-02-04 08:00:00 +09:00
categories:
  - ML
  - CS
tags:
pin: true
---

I personally organized the importants parts of the lecture in handwriting. The details for the mathematical derivation can be found in the lecture notes (which is public).
I will provide my handwritten notes here. (Maybe, i will type the contents in the notes? - If I have sufficient time..)

**Each Lecture covers the following topics:**

Lecture 1: Intro

Lecture 2, 3, 4: 
Linear Regression and its generalization. It first covers Linear Regression and Gradient Descent. It then covers Locally weighted & Logistic Regression. Finally, it covers GLMs (Generalized Linear Models). 
The important part is that regression models can be generalized, and you can choose which model to use according to the value you are predicting.

Lecture 5 & 6(front): 
GDA & Naive Bayes. It provides an another way to design models, predicting $p(x|y)$ and using the Bayes Rule.

Lecture 6(back) & 7:
SVM. It provides the idea of SVM and the kernel trick used in SVMs. 





