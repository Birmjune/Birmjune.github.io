---
title: [CS229] Outline + Notes
date: 2025-02-04 08:00:00 +09:00
categories: [ML]
tags: []       
math: true
layout: post
author: birm
---

I personally organized the important parts of the lecture.    
The details for the mathematical derivation can be found in the lecture notes (which is public). I will try to provide my handwritten notes here.

**Each Lecture covers the following topics:**

**Lecture 1:** Intro

**Lecture 2, 3, 4:** Linear Regression and its generalization. It first covers Linear Regression and Gradient Descent. It then covers Locally weighted & Logistic Regression. Finally, it covers GLMs (Generalized Linear Models).  
The important part is that regression models can be generalized, and you can choose which model to use according to the value you are predicting.

**Lecture 5 & 6(front):** GDA & Naive Bayes. It provides another way to design models by first modeling  
$p(x\mid y)$ and $p(y)$,
and then computing $p(y\mid x)$ as

$$
p(y\mid x) = \frac{p(x\mid y)p(y)}{p(x)}
$$  

to make predictions using Bayesâ€™ Rule.

**Lecture 6(back) & 7:** SVM. It provides the idea of SVM and the kernel trick used in SVMs.
